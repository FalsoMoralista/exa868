{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3309,
   "id": "98f8f7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3364,
   "id": "7edbd1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Neuron:\n",
    "    def __init__(self, input_shape):\n",
    "      self.input_shape = input_shape\n",
    "      self.bias = 1      \n",
    "      self.bias_weight = np.random.uniform(low=-1, high=1.0, size=1)[0]\n",
    "      self.weight_matrix = np.random.uniform(low=-1, high=1.0, size=self.input_shape)\n",
    "\n",
    "    def sigmoid(self,x):\n",
    "      return 1/(1+np.exp(-x))\n",
    "    \n",
    "    def threshold(self, x):\n",
    "      if x >= 0:\n",
    "        return 1\n",
    "      else:\n",
    "        return -1\n",
    "    \n",
    "    def dot_product(self, input_pattern): # todo: move inside layer class\n",
    "      return np.dot(input_pattern, self.weight_matrix)\n",
    "        \n",
    "    def activate(self, x, activation):\n",
    "      if activation == 'linear':\n",
    "        return x\n",
    "      if activation == 'sigmoid':\n",
    "        return self.sigmoid(x)\n",
    "      if activation == 'tanh':\n",
    "        return np.tanh(x)\n",
    "      if activation == 'threshold':\n",
    "        return self.threshold(x)\n",
    "\n",
    "class Layer():\n",
    "  def __init__(self, input_shape): \n",
    "    self.neurons = []\n",
    "    self.input_shape = input_shape\n",
    "    for i in range(input_shape[0]):\n",
    "      self.neurons.append(Neuron(input_shape=input_shape[1]))\n",
    "        \n",
    "class MultiLayerPerceptron: \n",
    "  \n",
    "  def build_layers(self, input_shape):\n",
    "    for i in range(0, len(input_shape)-1):\n",
    "      self.layers.append(Layer(input_shape=(input_shape[i+1],input_shape[i])))\n",
    "  \n",
    "  '''\n",
    "      input_shape: Array or tuple with the corresponding\n",
    "      number of neurons per layer.\n",
    "      \n",
    "  '''\n",
    "  def __init__(self, input_shape):\n",
    "    self.layers = []\n",
    "    self.build_layers(input_shape)\n",
    "    self.weight_matrix = [None] * len(input_shape)\n",
    "    self.weight_matrix[0] = np.random.uniform(low=-1, high=1.0, size=(input_shape[0]))\n",
    "    self.input_shape = input_shape\n",
    "\n",
    "  def derivative(self, x, activation):\n",
    "    if activation == 'tanh':\n",
    "      return (1 - (np.tanh(x)**2))\n",
    "    \n",
    "  def forward_pass(self, input_pattern):\n",
    "    first_out = []\n",
    "    inact_out = []\n",
    "    for neuron in self.layers[0].neurons:\n",
    "      dot = neuron.bias * neuron.bias_weight + neuron.dot_product(input_pattern) # \n",
    "      induced_local_field = neuron.activate(dot, 'tanh')\n",
    "      first_out.append(induced_local_field)\n",
    "      inact_out.append(dot)\n",
    "    output_by_layer = [first_out]\n",
    "    inactivated_outputs = [inact_out]\n",
    "    for i in range(len(self.layers)-1):\n",
    "      layer_out = []\n",
    "      inact_out = []\n",
    "      for neuron in self.layers[i+1].neurons:\n",
    "        dot = neuron.bias * neuron.bias_weight + neuron.dot_product(inactivated_outputs[i]) \n",
    "        inact_out.append(dot)\n",
    "        dot = neuron.bias * neuron.bias_weight + neuron.dot_product(output_by_layer[i])\n",
    "        induced_local_field = neuron.activate(dot, 'tanh')\n",
    "        layer_out.append(induced_local_field)\n",
    "      output_by_layer.append(layer_out)    \n",
    "      inactivated_outputs.append(inact_out)\n",
    "    return output_by_layer, inactivated_outputs \n",
    "  \n",
    "  def backpropagate(self, output_by_layer, inact_output_by_layer, learning_rate, input_pattern, desired_output):\n",
    "    total_error = []\n",
    "    lr = learning_rate # 0.01 # test \n",
    "    alpha = 0.9\n",
    "    for l in reversed(range(len(self.layers))):\n",
    "      # If it is an output layer:\n",
    "      if (l+1) == len(self.layers):\n",
    "        #print('Output layer')\n",
    "        for j in range(len(output_by_layer[l])):\n",
    "          neuron_output = output_by_layer[l][j]\n",
    "          error = (desired_output[j] - neuron_output) # (error**2)/2 \n",
    "          total_error.append(error)\n",
    "          #print('Error by neuron:', error, 'OUTPUT_LAYER')\n",
    "          # Update bias weight:\n",
    "          current_neuron = self.layers[l].neurons[j]\n",
    "          local_field_neuron_j = inact_output_by_layer[l][j]\n",
    "          delta = error * self.derivative(local_field_neuron_j, 'tanh')\n",
    "          momentum = alpha * delta\n",
    "          current_neuron.bias_weight = current_neuron.bias_weight + (lr * delta) + momentum\n",
    "          # Update weights to previous layer neurons:\n",
    "          for i in range(len(current_neuron.weight_matrix)):\n",
    "            y_sub_i = output_by_layer[l-1][i]\n",
    "            nabla_sub_ji = lr * delta * y_sub_i # nabla\n",
    "            current_neuron.weight_matrix[i] = current_neuron.weight_matrix[i] + nabla_sub_ji\n",
    "          # nabla_ji = eta * delta_j(n) * y_i(n)\n",
    "          # where delta_j(n) equals to: e_j(n) * fi'_j(v_j(n)) which corresponds to the local gradient\n",
    "          # where y_i(n) equals to fi_i(v_i(n)) which corresponds to the input signal of neuron j           \n",
    "      else:\n",
    "        for j in range(len(output_by_layer[l])): \n",
    "          accumulated_delta_k = 0\n",
    "          # Code below assumes that the (next) layer l+1 is the output layer therefore should only work for 2 layers networks at first.\n",
    "          # Despite that, making it generalizable should be straightforward since implies \n",
    "          # only testing whether layer 'l+1' is an output node or not (TO-DO).\n",
    "          for k in range(len(output_by_layer[l+1])):  # For each neuron k (if output layer) do:\n",
    "            neuron_output = output_by_layer[l+1][k]\n",
    "            error = (desired_output[k] - neuron_output)\n",
    "            local_field_neuron_k = inact_output_by_layer[l+1][k]\n",
    "            delta_sub_k = error * self.derivative(local_field_neuron_k, 'tanh') # Get the weighted sum of the local gradients by the\n",
    "            w_sub_kj = self.layers[l+1].neurons[k].weight_matrix[j] # corresponding weight connections between neurons j and k.\n",
    "            accumulated_delta_k += delta_sub_k * w_sub_kj  # Propagate them back as error for updating the weights on neuron j.\n",
    "          # Weight Update Rule:\n",
    "          local_field_neuron_j = inact_output_by_layer[l][j]\n",
    "          fi_sub_j = self.derivative(local_field_neuron_j, 'tanh')\n",
    "          delta_j  =  fi_sub_j * accumulated_delta_k\n",
    "          # Update weights:\n",
    "          neuron_j = self.layers[l].neurons[j]\n",
    "          for i in range(len(neuron_j.weight_matrix)):\n",
    "            # TODO: add momentum \n",
    "            nabla = lr * (delta_j * input_pattern[i])\n",
    "            # w -> (neuron_j.weight_matrix[i]) + nabla\n",
    "            momentum = alpha * nabla\n",
    "            neuron_j.weight_matrix[i] = neuron_j.weight_matrix[i] + nabla + momentum\n",
    "          # Update bias:\n",
    "          neuron_j.bias_weight += delta_j * lr\n",
    "    total_energy = 0\n",
    "    for error in total_error:\n",
    "      total_energy += (error**2)\n",
    "    return total_energy/2\n",
    "          \n",
    "  def test(self, X, y):\n",
    "    predictions = []\n",
    "    labels = []\n",
    "    for (x_test, y_test) in zip(X,y):\n",
    "      out, inact = self.forward_pass(x_test)\n",
    "      predictions.append(np.argmax(out[len(out)-1]))\n",
    "      labels.append(np.argmax(y_test))\n",
    "    acc = accuracy_score(y_true=labels, y_pred=predictions)\n",
    "    return acc    \n",
    "  \n",
    "  def train(self, X, y, learning_rate, error):\n",
    "    prev_mse = 999999999\n",
    "    current_mse  = 0\n",
    "    epochs = 0\n",
    "    while (abs(current_mse - prev_mse) > error):\n",
    "      prev_mse = current_mse\n",
    "      mse = 0\n",
    "      for (X_input, y_input) in zip(X,y):\n",
    "        out, inactivated_out = self.forward_pass(X_input)\n",
    "        mse += self.backpropagate(out, inactivated_out, learning_rate, X_input, y_input)\n",
    "      current_mse = (mse / len(X))\n",
    "      epochs += 1\n",
    "    print('Épocas:', epochs, ' | | Final MSE:', current_mse)\n",
    "    print('Training Accuracy:', self.test(X,y))    \n",
    "#nn = MultiLayerPerceptron(input_shape=[4,5,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3321,
   "id": "51e9fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assembly_dataset():\n",
    "  f = open(\"iris.data\", \"r\")\n",
    "  data_x = []\n",
    "  data_y = []\n",
    "  for line in f:\n",
    "    if len(line) != 1:\n",
    "      data = line.replace('\\n', '')\n",
    "      t = data.split(',')\n",
    "      data_y.append(t.pop(4))\n",
    "      data_x.append(np.array(t,dtype=np.float32))\n",
    "  return data_x, data_y\n",
    "\n",
    "def normalize_row(row, max_val, min_val):\n",
    "  for i in range(len(row)):\n",
    "    row[i] = (2 * ((row[i] - min_val)/(max_val - min_val))) - 1 \n",
    "\n",
    "def find_max_min(features):\n",
    "  max_val = features[0][0]\n",
    "  min_val = features[0][0]\n",
    "  for i in range(len(features)):\n",
    "    max_test = features[i][np.argmax(features[i])] \n",
    "    if max_test > max_val:\n",
    "      max_val = max_test\n",
    "      \n",
    "    min_test = features[i][np.argmin(features[i])] \n",
    "    if min_test < min_val:\n",
    "      min_val = min_test\n",
    "  return max_val, min_val\n",
    "    \n",
    "def feature_normalization(features):\n",
    "  max_val, min_val = find_max_min(features)\n",
    "  for feature in features:\n",
    "    normalize_row(feature, max_val, min_val)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f935ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def preprocess_dataset():  \n",
    "  features, labels = assembly_dataset()\n",
    "  feature_normalization(features)  \n",
    "\n",
    "  data_dictionary = {'Iris-virginica' : [], 'Iris-versicolor': [], 'Iris-setosa': []}\n",
    "\n",
    "  one_hot_label_dictionary = {'Iris-virginica' : [1,0,0], 'Iris-versicolor': [0,1,0], 'Iris-setosa': [0,0,1]}\n",
    "\n",
    "  for t in zip(features,labels):\n",
    "    data_dictionary[t[1]].append(t[0])\n",
    "  return data_dictionary, one_hot_label_dictionary\n",
    "data, one_hot_label_dictionary = preprocess_dataset()\n",
    "print(data)\n",
    "print(one_hot_label_dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3367,
   "id": "2a0a49ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "Épocas: 208  | | Final MSE: 0.04108329551403673\n",
      "Training Accuracy: 0.8740740740740741\n",
      "Validation Accuracy: 1.0\n",
      "Fold 1:\n",
      "Épocas: 162  | | Final MSE: 0.04258182273513295\n",
      "Training Accuracy: 0.8592592592592593\n",
      "Validation Accuracy: 0.9333333333333333\n",
      "Fold 2:\n",
      "Épocas: 181  | | Final MSE: 0.03749008001017502\n",
      "Training Accuracy: 0.8740740740740741\n",
      "Validation Accuracy: 0.8666666666666667\n",
      "Fold 3:\n",
      "Épocas: 229  | | Final MSE: 0.040252677316659255\n",
      "Training Accuracy: 0.8222222222222222\n",
      "Validation Accuracy: 0.8\n",
      "Fold 4:\n",
      "Épocas: 792  | | Final MSE: 0.33203747761997354\n",
      "Training Accuracy: 0.9555555555555556\n",
      "Validation Accuracy: 0.9333333333333333\n",
      "Fold 5:\n",
      "Épocas: 4369  | | Final MSE: 0.4054485706039565\n",
      "Training Accuracy: 0.9851851851851852\n",
      "Validation Accuracy: 0.9333333333333333\n",
      "Fold 6:\n",
      "Épocas: 174  | | Final MSE: 0.16056142373783264\n",
      "Training Accuracy: 0.674074074074074\n",
      "Validation Accuracy: 0.6666666666666666\n",
      "Fold 7:\n",
      "Épocas: 3188  | | Final MSE: 0.4005499965744236\n",
      "Training Accuracy: 0.9185185185185185\n",
      "Validation Accuracy: 0.9333333333333333\n",
      "Fold 8:\n",
      "Épocas: 1939  | | Final MSE: 0.4648488779200225\n",
      "Training Accuracy: 0.9555555555555556\n",
      "Validation Accuracy: 1.0\n",
      "Fold 9:\n",
      "Épocas: 141  | | Final MSE: 0.04387755665379253\n",
      "Training Accuracy: 0.9185185185185185\n",
      "Validation Accuracy: 0.7333333333333333\n",
      "Average validation accuracy: 0.8799999999999999\n",
      "Standard Deviation: 0.10666666666666667\n"
     ]
    }
   ],
   "source": [
    " # stratified random sampling\n",
    "labels = list(one_hot_label_dictionary.keys())\n",
    "X = []\n",
    "y = []\n",
    "for i in range(len(labels)):\n",
    "  for x_sample in data[labels[i]]:\n",
    "    X.append(x_sample)\n",
    "    y.append(i)\n",
    "    \n",
    "labels = ['Iris-virginica', 'Iris-versicolor', 'Iris-setosa']\n",
    "skf = StratifiedKFold(n_splits=10) # shuffle and configure seed.\n",
    "\n",
    "avg = []\n",
    "\n",
    "#skf.get_n_splits(X, y)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "  nn = MultiLayerPerceptron(input_shape=[4,5,3])\n",
    "  print(f\"Fold {i}:\")\n",
    "  #print(f\"  Train: index={train_index}\")\n",
    "  #print(f\"  Test:  index={test_index}\")\n",
    "  train_X = []\n",
    "  train_y = []\n",
    "  for j in train_index:\n",
    "    train_X.append(X[j])\n",
    "    train_y.append(one_hot_label_dictionary[labels[y[j]]])\n",
    "\n",
    "  validation_X = []\n",
    "  validation_y = []\n",
    "  for j in test_index:\n",
    "    validation_X.append(X[j])\n",
    "    validation_y.append((one_hot_label_dictionary[labels[y[j]]]))\n",
    "  \n",
    "  nn.train(train_X, train_y, learning_rate=0.1, error=1.0e-6)\n",
    "  val_acc = nn.test(validation_X, validation_y)\n",
    "  print('Validation Accuracy:', val_acc)\n",
    "  avg.append(val_acc)\n",
    "print('Average validation accuracy:', np.average(avg))\n",
    "print('Standard Deviation:', np.std(avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009a49cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
